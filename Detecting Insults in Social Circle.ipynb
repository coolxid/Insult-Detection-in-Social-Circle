{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "############### Reading & Removing Noise from comments #################\n",
    "def read_training_data(training_file):\n",
    "    f = open(training_file)\n",
    "    f.readline()\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for row in f:\n",
    "        row = row.strip().split(\"\\\"\\\"\\\"\")\n",
    "        label = row[0].split(\",\")[0]\n",
    "        text = row[1]       \n",
    "        data.append(re.sub('_|\\.',' ',re.sub(\"\\n|,|\\'|\\\"\",\"\",text)))\n",
    "        labels.append(float(label))\n",
    "    return {\"data\":data,\"labels\":labels}\n",
    "\n",
    "def read_test_data1(test_file):\n",
    "    f = open(test_file)\n",
    "    f.readline()\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for row in f:\n",
    "        \n",
    "        row = row.strip().split(\"\\\"\\\"\\\"\")\n",
    "        label = row[0].split(\",\")[0]\n",
    "        text = row[1]       \n",
    "        data.append(re.sub('_|\\.',' ',re.sub(\"\\n|,|\\'|\\\"\",\"\",text)))\n",
    "        labels.append(float(label))\n",
    "    return {\"data\":data,\"labels\":labels}\n",
    "\n",
    "def read_test_data2(test_file):\n",
    "    f = open(test_file)\n",
    "    f.readline()\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    for row in f:\n",
    "        \n",
    "        row = row.strip().split(\"\\\"\\\"\\\"\")\n",
    "        label = row[0].split(\",\")[1]\n",
    "        text = row[1]       \n",
    "        data.append(re.sub('_|\\.',' ',re.sub(\"\\n|,|\\'|\\\"\",\"\",text)))\n",
    "        labels.append(float(label))\n",
    "    return {\"data\":data,\"labels\":labels}\n",
    "\n",
    "############### Feature Engineering ######################################\n",
    "def insultWords(text):\n",
    "    f = open('Bad words.txt')\n",
    "    f.readline()\n",
    "    count=0\n",
    "    data=\"\"\n",
    "    for row in f:\n",
    "        text = row       \n",
    "        if count==0:\n",
    "            data=re.sub('_|\\.',' ',re.sub(\"\\n|,|\\'|\\\"\",\"\",text))\n",
    "            count+=1\n",
    "        else:\n",
    "            data=\"|\" + re.sub('_|\\.',' ',re.sub(\"\\n|,|\\'|\\\"\",\"\",text))\n",
    "    return len(re.findall(data,text.lower() ))\n",
    "    \n",
    "def negWords(text):\n",
    "    f = open('neg words.txt')\n",
    "    f.readline()\n",
    "    count=0\n",
    "    data=\"\"\n",
    "    for row in f:\n",
    "        text = row       \n",
    "        if count==0:\n",
    "            data=re.sub('_|\\.',' ',re.sub(\"\\n|,|\\'|\\\"\",\"\",text))\n",
    "            count+=1\n",
    "        else:\n",
    "            data=\"|\" + re.sub('_|\\.',' ',re.sub(\"\\n|,|\\'|\\\"\",\"\",text))\n",
    "    return len(re.findall(data,text.lower() ))\n",
    "\n",
    "def exaggeration(text):\n",
    "    return (len(re.findall(\"\\?|!\",text )))\n",
    "\n",
    "def extract_features(texts, feature_functions):\n",
    "    return [[f(es) for f in feature_functions] for es in texts]   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "Training Completed\n"
     ]
    }
   ],
   "source": [
    "#################### Training ###################################\n",
    "print(\"Reading Training Data\")\n",
    "training = read_training_data(\"data/train.csv\")\n",
    "\n",
    "#################### Extract Features ###########################\n",
    "feature_functions = [insultWords,negWords,exaggeration]\n",
    "features = extract_features(training[\"data\"],feature_functions)\n",
    "\n",
    "#################### Calculating TFIDF Vector ###################   \n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2),token_pattern=ur'\\b\\w+\\b',stop_words=None, min_df=3)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(training[\"data\"])\n",
    "        \n",
    "#################### LogisticRegression #########################    \n",
    "lr = LogisticRegression(C=.1, class_weight=None, dual=False,fit_intercept=True, intercept_scaling=1, penalty='l2',tol=0.0001)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),token_pattern=ur'\\b\\w+\\b',stop_words=None, min_df=3,binary=True)\n",
    "text_features = bigram_vectorizer.fit_transform(training[\"data\"]) \n",
    "lr.fit(text_features,training[\"labels\"])\n",
    "lr_preds = lr.predict_proba(text_features)\n",
    "\n",
    "#################### Multinomial Naive Bayes ####################\n",
    "nb = MultinomialNB()\n",
    "nb.fit(tfidf_features, training[\"labels\"])\n",
    "nb_preds = nb.predict_proba(tfidf_features)  \n",
    "\n",
    "#################### RandomForest Classifiers ###################\n",
    "rf = RandomForestClassifier(n_estimators=500)\n",
    "rf.fit(features,training[\"labels\"]) \n",
    "rf_preds = rf.predict_proba(features)\n",
    "\n",
    "#################### GradientBoosting Classifier ################\n",
    "gb_features = np.empty((len(lr_preds),2))\n",
    "for i in range(len(lr_preds)):\n",
    "    gb_features[i][0] = (lr_preds[i][1])\n",
    "    #gb_features[i][1] = (rf_preds[i][1])\n",
    "    gb_features[i][1] = (nb_preds[i][1])\n",
    "    \n",
    "gb = GradientBoostingClassifier(n_estimators=200)\n",
    "gb.fit(gb_features,training[\"labels\"])\n",
    "\n",
    "print \"Training Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Test Data\n",
      "Matched: 2233  Outof: 2647\n",
      "Test Set-1 Accuracy =  0.843596524367\n"
     ]
    }
   ],
   "source": [
    "#################### Testing DataSet1 ####################################\n",
    "print(\"Reading Test Data\")\n",
    "test = read_test_data1(\"data/test_with_solutions.csv\")\n",
    "\n",
    "text_features_test = bigram_vectorizer.transform(test[\"data\"])    \n",
    "tfidf_features_test = tfidf_vectorizer.transform(test[\"data\"])\n",
    "features = extract_features(test[\"data\"],feature_functions)\n",
    "\n",
    "lr_preds = lr.predict_proba(text_features_test)\n",
    "rf_preds = rf.predict_proba(features) \n",
    "nb_preds = nb.predict_proba(tfidf_features_test)\n",
    "\n",
    "gb_features = np.empty((len(lr_preds),2))\n",
    "   \n",
    "for i in range(len(lr_preds)):\n",
    "    gb_features[i][0] = (lr_preds[i][1])\n",
    "    #gb_features[i][1] = (rf_preds[i][1])\n",
    "    gb_features[i][1] = (nb_preds[i][1])\n",
    "    \n",
    "predictions = gb.predict_proba(gb_features)\n",
    "predict=np.zeros(len(predictions))\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i][1] >= predictions[i][0]):\n",
    "        predict[i]=1\n",
    "    else:\n",
    "        predict[i]=0\n",
    "\n",
    "print \"Matched:\",np.sum(predict==test[\"labels\"]),\" Outof:\",len(test[\"labels\"])\n",
    "print \"Test Set-1 Accuracy = \", np.sum(predict==test[\"labels\"])/float(len(test[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 2219  Outof: 2647\n",
      "Logistic Regression Test Set-1 Accuracy =  0.838307517945\n"
     ]
    }
   ],
   "source": [
    "predict=np.zeros(len(lr_preds))\n",
    "for i in range(len(lr_preds)):\n",
    "    if (lr_preds[i][1] >= lr_preds[i][0]):\n",
    "        predict[i]=1\n",
    "    else:\n",
    "        predict[i]=0\n",
    "print \"Matched:\",np.sum(predict==test[\"labels\"]),\" Outof:\",len(test[\"labels\"])\n",
    "print \"Logistic Regression Test Set-1 Accuracy = \", np.sum(predict==test[\"labels\"])/float(len(test[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 2089  Outof: 2647\n",
      "Naive Bayes Test Set-1 Accuracy =  0.789195315451\n"
     ]
    }
   ],
   "source": [
    "predict=np.zeros(len(nb_preds))\n",
    "for i in range(len(nb_preds)):\n",
    "    if (nb_preds[i][1] >= nb_preds[i][0]):\n",
    "        predict[i]=1\n",
    "    else:\n",
    "        predict[i]=0\n",
    "print \"Matched:\",np.sum(predict==test[\"labels\"]),\" Outof:\",len(test[\"labels\"])\n",
    "print \"Naive Bayes Test Set-1 Accuracy = \", np.sum(predict==test[\"labels\"])/float(len(test[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 1952  Outof: 2647\n",
      "Random Forest Test Set-1 Accuracy =  0.737438609747\n"
     ]
    }
   ],
   "source": [
    "predict=np.zeros(len(rf_preds))\n",
    "for i in range(len(rf_preds)):\n",
    "    if (rf_preds[i][1] >= rf_preds[i][0]):\n",
    "        predict[i]=1\n",
    "    else:\n",
    "        predict[i]=0\n",
    "print \"Matched:\",np.sum(predict==test[\"labels\"]),\" Outof:\",len(test[\"labels\"])\n",
    "print \"Random Forest Test Set-1 Accuracy = \", np.sum(predict==test[\"labels\"])/float(len(test[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Test Data\n",
      "Matched: 1584  Outof: 2235\n",
      "Test Set-2 Accuracy =  0.708724832215\n"
     ]
    }
   ],
   "source": [
    "#################### Testing DataSet2 ####################################\n",
    "print(\"Reading Test Data\")\n",
    "test = read_test_data2(\"data/impermium_verification_labels.csv\")\n",
    "\n",
    "text_features_test = bigram_vectorizer.transform(test[\"data\"])    \n",
    "tfidf_features_test = tfidf_vectorizer.transform(test[\"data\"])\n",
    "features = extract_features(test[\"data\"],feature_functions)\n",
    "\n",
    "lr_preds = lr.predict_proba(text_features_test)\n",
    "rf_preds = rf.predict_proba(features) \n",
    "nb_preds = nb.predict_proba(tfidf_features_test)\n",
    "\n",
    "gb_features = np.empty((len(lr_preds),2))\n",
    "      \n",
    "for i in range(len(lr_preds)):\n",
    "    gb_features[i][0] = (lr_preds[i][1])\n",
    "    #gb_features[i][1] = (rf_preds[i][1])\n",
    "    gb_features[i][1] = (nb_preds[i][1]) \n",
    "       \n",
    "predictions = gb.predict_proba(gb_features)\n",
    "predict=np.zeros(len(predictions))\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i][1] >= predictions[i][0]):\n",
    "        predict[i]=1\n",
    "    else:\n",
    "        predict[i]=0\n",
    "print \"Matched:\",np.sum(predict==test[\"labels\"]),\" Outof:\",len(test[\"labels\"])\n",
    "print \"Test Set-2 Accuracy = \", np.sum(predict==test[\"labels\"])/float(len(test[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 1515  Outof: 2235\n",
      "Logistic Regression Test Set-2 Accuracy =  0.677852348993\n"
     ]
    }
   ],
   "source": [
    "predict=np.zeros(len(lr_preds))\n",
    "for i in range(len(lr_preds)):\n",
    "    if (lr_preds[i][1] >= lr_preds[i][0]):\n",
    "        predict[i]=1\n",
    "    else:\n",
    "        predict[i]=0\n",
    "print \"Matched:\",np.sum(predict==test[\"labels\"]),\" Outof:\",len(test[\"labels\"])\n",
    "print \"Logistic Regression Test Set-2 Accuracy = \", np.sum(predict==test[\"labels\"])/float(len(test[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 1156  Outof: 2235\n",
      "Random Forest Test Set-2 Accuracy =  0.517225950783\n"
     ]
    }
   ],
   "source": [
    "predict=np.zeros(len(rf_preds))\n",
    "for i in range(len(rf_preds)):\n",
    "    if (rf_preds[i][1] >= rf_preds[i][0]):\n",
    "        predict[i]=1\n",
    "    else:\n",
    "        predict[i]=0\n",
    "print \"Matched:\",np.sum(predict==test[\"labels\"]),\" Outof:\",len(test[\"labels\"])\n",
    "print \"Random Forest Test Set-2 Accuracy = \", np.sum(predict==test[\"labels\"])/float(len(test[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: 1351  Outof: 2235\n",
      "Naive Bayes Test Set-2 Accuracy =  0.604474272931\n"
     ]
    }
   ],
   "source": [
    "predict=np.zeros(len(nb_preds))\n",
    "for i in range(len(nb_preds)):\n",
    "    if (nb_preds[i][1] >= nb_preds[i][0]):\n",
    "        predict[i]=1\n",
    "    else:\n",
    "        predict[i]=0\n",
    "print \"Matched:\",np.sum(predict==test[\"labels\"]),\" Outof:\",len(test[\"labels\"])\n",
    "print \"Naive Bayes Test Set-2 Accuracy = \", np.sum(predict==test[\"labels\"])/float(len(test[\"labels\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
